{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa438a6-38e3-461a-9296-df0386a10df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "import scipy.signal as signal\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211350cf-e225-4c76-9e9c-52df890b8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioPreprocessing class\n",
    "class AudioPreprocessing:\n",
    "    def __init__(self, sample_rate=16000, frame_size=0.025, frame_stride=0.01):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_stride = frame_stride\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "        return audio, sr\n",
    "\n",
    "    def noise_removal(self, audio):\n",
    "        b, a = signal.butter(1, 100 / (0.5 * self.sample_rate), btype='high')\n",
    "        audio_denoised = signal.lfilter(b, a, audio)\n",
    "        return audio_denoised\n",
    "\n",
    "    def silence_removal(self, audio, top_db=20):\n",
    "        non_silent_intervals = librosa.effects.split(audio, top_db=top_db)\n",
    "        audio_nonsilent = np.concatenate([audio[start:end] for start, end in non_silent_intervals])\n",
    "        return audio_nonsilent\n",
    "\n",
    "    def normalize(self, audio):\n",
    "        return librosa.util.normalize(audio)\n",
    "\n",
    "    def resample(self, audio, orig_sr, target_sr):\n",
    "        if orig_sr != target_sr:\n",
    "            audio_resampled = librosa.resample(audio, orig_sr=orig_sr, target_sr=target_sr)\n",
    "            return audio_resampled\n",
    "        return audio\n",
    "\n",
    "    def preprocess(self, file_path):\n",
    "        audio, sr = self.load_audio(file_path)\n",
    "        audio = self.resample(audio, sr, self.sample_rate)\n",
    "        audio = self.noise_removal(audio)\n",
    "        audio = self.silence_removal(audio)\n",
    "        audio = self.normalize(audio)\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2649e818-a5f2-430f-9c4f-1c7894cb5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioFeatureExtractor class\n",
    "class AudioFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = AudioPreprocessing()\n",
    "\n",
    "    def extract_audio_features(self, file_path):\n",
    "        try:\n",
    "            # Apply preprocessing before extracting features\n",
    "            preprocessed_audio = self.preprocessor.preprocess(file_path)\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=preprocessed_audio, sr=self.preprocessor.sample_rate, n_mfcc=40)\n",
    "            mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "\n",
    "            return mfccs_mean\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio file: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef3b605-bfdb-4571-b920-e8a6a709cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioEmotionTester class\n",
    "class AudioEmotionTester:\n",
    "    def __init__(self):\n",
    "        self.model = load_model('emotion_recognition_model.h5')  # Load your trained model\n",
    "        self.feature_extractor = AudioFeatureExtractor()\n",
    "        self.scaler = joblib.load('scaler.pkl')\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def load_label_encoder(self, encoder_path):\n",
    "        self.label_encoder.classes_ = np.load(encoder_path, allow_pickle=True)\n",
    "\n",
    "    def preprocess_features(self, features):\n",
    "        if len(features.shape) == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return features_scaled.reshape(features_scaled.shape[0], 1, features_scaled.shape[1])\n",
    "\n",
    "    def predict_emotion(self, features):\n",
    "        processed_features = self.preprocess_features(features)\n",
    "        prediction = self.model.predict(processed_features)\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_label = self.label_encoder.inverse_transform(predicted_class)\n",
    "        return predicted_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f3d71e-3a7a-407d-84e2-0a631426ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle audio file upload and prediction\n",
    "def predict_emotion(audio_file):\n",
    "    # Save the uploaded file temporarily\n",
    "    with open(\"uploaded_audio.wav\", \"wb\") as f:\n",
    "        f.write(audio_file.read())\n",
    "\n",
    "    # Extract features and predict emotion\n",
    "    feature_extractor = AudioFeatureExtractor()\n",
    "    extracted_features = feature_extractor.extract_audio_features(\"uploaded_audio.wav\")\n",
    "\n",
    "    if extracted_features is not None:\n",
    "        tester = AudioEmotionTester()\n",
    "        tester.load_label_encoder('label_encoder_classes.npy')\n",
    "        predicted_emotion = tester.predict_emotion(extracted_features)\n",
    "        print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "    else:\n",
    "        print(\"Failed to extract audio features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df87706d-3a1c-46a0-b5d7-304d614ab365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2fd183d9f441cf9d88f23c672871bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.wav,.mp3,.ogg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b298c5eb391749a19260872883e2c283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict Emotion', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03136220c0d47b58194a85ab12171f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a file upload widget\n",
    "upload_widget = widgets.FileUpload(accept='.wav,.mp3,.ogg', multiple=False)\n",
    "\n",
    "# Button to trigger prediction\n",
    "button = widgets.Button(description=\"Predict Emotion\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if upload_widget.value:\n",
    "            # Get the first uploaded file\n",
    "            audio_file = next(iter(upload_widget.value.values()))\n",
    "            predict_emotion(audio_file['content'])\n",
    "        else:\n",
    "            print(\"Please upload an audio file.\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the upload widget and button\n",
    "display(upload_widget, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9754d-12d1-4f95-8b2f-6a6e71f48187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
